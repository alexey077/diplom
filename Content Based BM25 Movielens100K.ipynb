{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c35f484",
   "metadata": {},
   "source": [
    "Recommender Net with Label Encoder Movielens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install rank_bm25\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import ndcg_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt_tab')  # corrected from 'punkt_tab'\n",
    "\n",
    "# Load datasets\n",
    "movies = pd.read_csv('movies.csv')\n",
    "tags = pd.read_csv('tags.csv')\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "\n",
    "# Merge tags into movies\n",
    "tagged = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "movies = pd.merge(movies, tagged, on='movieId', how='left')\n",
    "movies['tag'] = movies['tag'].fillna('')\n",
    "\n",
    "# Combine genres and tags into one metadata string\n",
    "movies['metadata'] = movies['genres'].str.replace('|', ' ', regex=False) + ' ' + movies['tag']\n",
    "\n",
    "# Tokenize metadata for BM25\n",
    "tokenized_corpus = movies['metadata'].apply(lambda x: word_tokenize(x.lower())).tolist()\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "movie_indices = pd.Series(movies.index, index=movies['movieId'])\n",
    "\n",
    "# Merge ratings into movies (average rating per movie)\n",
    "avg_ratings = ratings.groupby('movieId')['rating'].mean().reset_index()\n",
    "avg_ratings.columns = ['movieId', 'avg_rating']\n",
    "movies = pd.merge(movies, avg_ratings, on='movieId', how='left')\n",
    "movies['avg_rating'] = movies['avg_rating'].fillna(0)\n",
    "\n",
    "# Recommendation function based on BM25 and rating boost\n",
    "def content_based_recommendations(movie_id, top_n=10, rating_weight=0.2):\n",
    "    idx = movie_indices[movie_id]\n",
    "    query = tokenized_corpus[idx]\n",
    "    bm25_scores = bm25.get_scores(query)\n",
    "\n",
    "    # Enhance BM25 score with average rating\n",
    "    enhanced_scores = []\n",
    "    for i, score in enumerate(bm25_scores):\n",
    "        rating_boost = rating_weight * movies.iloc[i]['avg_rating'] / 5\n",
    "        enhanced_scores.append((i, score + rating_boost))\n",
    "\n",
    "    enhanced_scores = sorted(enhanced_scores, key=lambda x: x[1], reverse=True)[1:top_n+1]\n",
    "    movie_indices_rec = [i[0] for i in enhanced_scores]\n",
    "    return movie_indices_rec\n",
    "\n",
    "# Evaluation with NDCG\n",
    "def evaluate_ndcg(sample_size=100, top_n=10):\n",
    "    sample = ratings[ratings['rating'] >= 4.0].groupby('userId').sample(n=1, random_state=42)\n",
    "    ndcg_scores = []\n",
    "\n",
    "    for _, row in sample.iterrows():\n",
    "        movie_id = row['movieId']\n",
    "        relevant_idx = movie_indices.get(movie_id)\n",
    "        if pd.isna(relevant_idx):\n",
    "            continue\n",
    "\n",
    "        recommendations = content_based_recommendations(movie_id, top_n=top_n)\n",
    "\n",
    "        y_true = np.zeros((1, len(movies)))\n",
    "        y_score = np.zeros((1, len(movies)))\n",
    "\n",
    "        y_true[0, int(relevant_idx)] = 1\n",
    "        for rank, idx in enumerate(recommendations):\n",
    "            y_score[0, idx] = top_n - rank\n",
    "\n",
    "        score = ndcg_score(y_true, y_score)\n",
    "        ndcg_scores.append(score)\n",
    "\n",
    "    return np.mean(ndcg_scores) if ndcg_scores else 0\n",
    "\n",
    "# Example usage\n",
    "ndcg_result = evaluate_ndcg(sample_size=100, top_n=10)\n",
    "print(f\"Average NDCG@10 Score using BM25: {ndcg_result:.4f}\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "def additional_metrics(sample_size=100, top_k=10):\n",
    "    sample = ratings[ratings['rating'] >= 4.0].groupby('userId').sample(n=1, random_state=42)\n",
    "\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "\n",
    "    ndcg_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for _, row in sample.iterrows():\n",
    "        user_movie_id = row['movieId']\n",
    "        user_rating = row['rating']\n",
    "        relevant_idx = movie_indices.get(user_movie_id)\n",
    "\n",
    "        if pd.isna(relevant_idx):\n",
    "            continue\n",
    "\n",
    "        rec_indices = content_based_recommendations(user_movie_id, top_n=top_k)\n",
    "\n",
    "        for idx in rec_indices:\n",
    "            y_pred_all.append(movies.iloc[idx]['avg_rating'])\n",
    "            y_true_all.append(user_rating)  # Ground truth is the high rating (â‰¥4.0)\n",
    "\n",
    "        # Binary relevance vector\n",
    "        y_true_bin = np.zeros(len(movies))\n",
    "        y_true_bin[int(relevant_idx)] = 1\n",
    "\n",
    "        y_score_bin = np.zeros(len(movies))\n",
    "        for rank, idx in enumerate(rec_indices):\n",
    "            y_score_bin[idx] = top_k - rank\n",
    "\n",
    "        ndcg_scores.append(ndcg_score([y_true_bin], [y_score_bin]))\n",
    "\n",
    "        # Precision & Recall @K\n",
    "        hits = 1 if int(relevant_idx) in rec_indices else 0\n",
    "        precision_scores.append(hits / top_k)\n",
    "        recall_scores.append(hits / 1)  # Only one relevant item in this simplified case\n",
    "\n",
    "    # Error Metrics\n",
    "    rmse = sqrt(mean_squared_error(y_true_all, y_pred_all)) if y_true_all else 0\n",
    "    mae = mean_absolute_error(y_true_all, y_pred_all) if y_true_all else 0\n",
    "    mape = np.mean(np.abs((np.array(y_true_all) - np.array(y_pred_all)) / np.array(y_true_all))) * 100 if y_true_all else 0\n",
    "\n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        f'NDCG@{top_k}': np.mean(ndcg_scores),\n",
    "        f'Precision@{top_k}': np.mean(precision_scores),\n",
    "        f'Recall@{top_k}': np.mean(recall_scores)\n",
    "    }\n",
    "\n",
    "metrics_result = additional_metrics(sample_size=100, top_k=10)\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for metric, value in metrics_result.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862aadb",
   "metadata": {},
   "source": [
    "RMSE: 1.0541\n",
    "\n",
    "MAE: 0.8178\n",
    "\n",
    "MAPE: 18.1852\n",
    "\n",
    "NDCG@10: 0.1771\n",
    "\n",
    "Precision@10: 0.0171\n",
    "\n",
    "Recall@10: 0.1708"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
