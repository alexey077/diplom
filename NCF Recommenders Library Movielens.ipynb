{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c35f484",
   "metadata": {},
   "source": [
    "SASRec and SVD Hybrid Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score, precision_score, recall_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ratings = pd.read_csv('ratings.csv')\n",
    "movies_df = pd.read_csv('movies.csv')\n",
    "ratings['rating'] /= 5\n",
    "\n",
    "# Build user-item matrix\n",
    "user_item_matrix = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "\n",
    "# Prepare SVD model\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "user_factors = svd.fit_transform(user_item_matrix)\n",
    "item_factors = svd.components_.T\n",
    "reconstructed = np.dot(user_factors, item_factors.T)\n",
    "\n",
    "# Recommend top-N items for user 1\n",
    "user_index = 0  # userId = 1\n",
    "user_ratings = user_item_matrix.iloc[user_index].values\n",
    "reconstructed_scores = reconstructed[user_index]\n",
    "\n",
    "# Filter out already seen movies\n",
    "seen = user_ratings > 0\n",
    "reconstructed_scores[seen] = -np.inf\n",
    "\n",
    "# Get top-N recommendations\n",
    "top_n = 10\n",
    "top_indices = np.argsort(reconstructed_scores)[-top_n:][::-1]\n",
    "top_movie_ids = user_item_matrix.columns[top_indices]\n",
    "movie_titles = dict(zip(movies_df['movieId'], movies_df['title']))\n",
    "\n",
    "print(\"SVD Recommendations for user 1:\")\n",
    "for i, mid in enumerate(top_movie_ids, 1):\n",
    "    print(f\"{i}. {movie_titles.get(mid, f'Unknown ({mid})')}\")\n",
    "\n",
    "# Evaluate SVD model\n",
    "\n",
    "def evaluate_svd(user_item_matrix, reconstructed, top_k=10, sample_users=100):\n",
    "    hit, ndcg, precision, recall = [], [], [], []\n",
    "    users_with_enough = user_item_matrix[user_item_matrix.gt(0).sum(axis=1) > 5]\n",
    "    sampled_users = users_with_enough.sample(n=min(sample_users, len(users_with_enough)), random_state=42)\n",
    "\n",
    "    for user_id, row in sampled_users.iterrows():\n",
    "        true_items = row[row > 0].index.tolist()\n",
    "        if len(true_items) < 2:\n",
    "            continue\n",
    "        held_out = true_items[-1]\n",
    "        user_seen = row.copy()\n",
    "        user_seen[held_out] = 0\n",
    "\n",
    "        scores = reconstructed[user_id - 1].copy()\n",
    "        scores[user_seen > 0] = -np.inf\n",
    "\n",
    "        top_preds = np.argsort(scores)[::-1][:top_k]\n",
    "        held_out_idx = user_item_matrix.columns.get_loc(held_out)\n",
    "\n",
    "        hit.append(int(held_out_idx in top_preds))\n",
    "        rel = [1 if i == held_out_idx else 0 for i in top_preds]\n",
    "        ndcg.append(ndcg_score([rel], [list(range(top_k, 0, -1))]))\n",
    "\n",
    "        precision.append(np.sum(rel) / top_k)\n",
    "        recall.append(np.sum(rel))  # 1 relevant item\n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'Metric': ['Hit@10', 'NDCG@10', 'Precision@10', 'Recall@10'],\n",
    "        'Score': [np.mean(hit), np.mean(ndcg), np.mean(precision), np.mean(recall)]\n",
    "    })\n",
    "    print(\"\\nEvaluation Metrics Table:\")\n",
    "    print(results.to_string(index=False))\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_svd(user_item_matrix, reconstructed, top_k=10, sample_users=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862aadb",
   "metadata": {},
   "source": [
    "Metric    Score\n",
    "\n",
    "Hit@10 0.150000\n",
    "\n",
    "NDCG@10 0.120666\n",
    "\n",
    "Precision@10 0.015000\n",
    "\n",
    "Recall@10 0.150000"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
