{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c35f484",
   "metadata": {},
   "source": [
    "LightGBM + Vader GoogleMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q lightgbm tqdm\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "file_path = '/content/drive/MyDrive/review-South_Carolina_10.json'\n",
    "\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Check and handle missing columns\n",
    "expected_cols = {'user_id', 'gmap_id', 'rating', 'time', 'text'}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in dataset: {missing}\")\n",
    "\n",
    "df['text'] = df['text'].fillna('')\n",
    "\n",
    "# VADER Sentiment Analysis\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_sentiment_batch(texts, batch_size=10000):\n",
    "    sentiments = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"VADER Sentiment\", ncols=100):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        scores = [sia.polarity_scores(text)['compound'] for text in batch]\n",
    "        sentiments.extend(scores)\n",
    "    return sentiments\n",
    "\n",
    "df['vader_sentiment'] = vader_sentiment_batch(df['text'].tolist())\n",
    "\n",
    "# Time features\n",
    "df['time'] = pd.to_datetime(df['time'], unit='ms', errors='coerce')\n",
    "df['hour'] = df['time'].dt.hour\n",
    "df['weekday'] = df['time'].dt.weekday\n",
    "df['month'] = df['time'].dt.month\n",
    "\n",
    "def time_of_day(hour):\n",
    "    if pd.isna(hour):\n",
    "        return 'unknown'\n",
    "    if 5 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'evening'\n",
    "    else:\n",
    "        return 'night'\n",
    "\n",
    "df['time_of_day'] = df['hour'].apply(time_of_day).astype('category')\n",
    "df['is_weekend'] = df['weekday'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "latest_time = df['time'].max()\n",
    "df['days_since_review'] = (latest_time - df['time']).dt.days\n",
    "\n",
    "# Encode IDs\n",
    "df['user_id'] = df['user_id'].astype('category')\n",
    "df['gmap_id'] = df['gmap_id'].astype('category')\n",
    "\n",
    "# Define features for LightGBM\n",
    "features = [\n",
    "    'user_id', 'gmap_id', 'hour', 'weekday', 'month',\n",
    "    'is_weekend', 'time_of_day', 'days_since_review', 'vader_sentiment'\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['rating']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train LightGBM\n",
    "train_data = lgb.Dataset(\n",
    "    X_train, label=y_train,\n",
    "    categorical_feature=['user_id', 'gmap_id', 'time_of_day']\n",
    ")\n",
    "test_data = lgb.Dataset(\n",
    "    X_test, label=y_test,\n",
    "    reference=train_data,\n",
    "    categorical_feature=['user_id', 'gmap_id', 'time_of_day']\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "print(\"Training LightGBM...\")\n",
    "evals_result = {}\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    num_boost_round=100,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=10),\n",
    "        lgb.log_evaluation(period=10)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "print(f\"\\nTest RMSE: {rmse:.4f}\")\n",
    "\n",
    "#  Feature importance\n",
    "lgb.plot_importance(model, max_num_features=15, importance_type='gain')\n",
    "plt.show()\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Build interaction map from training set\n",
    "train_user_items = df.loc[X_train.index].groupby('user_id')['gmap_id'].apply(set).to_dict()\n",
    "all_items = df['gmap_id'].unique().tolist()\n",
    "test_users = df.loc[X_test.index]['user_id'].unique()\n",
    "\n",
    "K_list = [5, 10, 20]\n",
    "results_by_k = {k: {'recall': [], 'precision': [], 'hit': [], 'ndcg': []} for k in K_list}\n",
    "\n",
    "# Prepare categorical encodings if needed\n",
    "X_test_full = X.copy()\n",
    "X_test_full['rating'] = y\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Setup\n",
    "K_list = [5, 10, 20]\n",
    "results_by_k = {k: {'recall': [], 'precision': [], 'hit': [], 'ndcg': []} for k in K_list}\n",
    "all_items = df['gmap_id'].unique().tolist()\n",
    "train_user_items = df.loc[X_train.index].groupby('user_id')['gmap_id'].apply(set).to_dict()\n",
    "test_users = df.loc[X_test.index]['user_id'].unique()\n",
    "\n",
    "# Precompute features\n",
    "user_feats = df[['user_id', 'hour', 'weekday', 'month', 'is_weekend', 'time_of_day', 'days_since_review']].drop_duplicates('user_id').set_index('user_id')\n",
    "item_feats = df[['gmap_id', 'vader_sentiment']].drop_duplicates('gmap_id').set_index('gmap_id')\n",
    "\n",
    "# Prepare candidate rows\n",
    "candidate_rows = []\n",
    "labels = []\n",
    "\n",
    "print(\"\\n Preparing candidate batch...\")\n",
    "\n",
    "for user_id in tqdm(test_users, desc=\"Users\"):\n",
    "    if user_id not in train_user_items:\n",
    "        continue\n",
    "    user_seen = train_user_items[user_id]\n",
    "    user_df = X_test[X_test['user_id'] == user_id]\n",
    "    if user_df.empty:\n",
    "        continue\n",
    "\n",
    "    pos_item = user_df.sample(1, random_state=42)['gmap_id'].values[0]\n",
    "    negatives = [item for item in all_items if item not in user_seen and item != pos_item]\n",
    "    if len(negatives) < 99:\n",
    "        continue\n",
    "    sampled_negs = random.sample(negatives, 99)\n",
    "\n",
    "    candidate_items = [pos_item] + sampled_negs\n",
    "    user_data = user_feats.loc[user_id]\n",
    "\n",
    "    for item_id in candidate_items:\n",
    "        row = {\n",
    "            'user_id': user_id,\n",
    "            'gmap_id': item_id,\n",
    "            'hour': user_data['hour'],\n",
    "            'weekday': user_data['weekday'],\n",
    "            'month': user_data['month'],\n",
    "            'is_weekend': user_data['is_weekend'],\n",
    "            'time_of_day': user_data['time_of_day'],\n",
    "            'days_since_review': user_data['days_since_review'],\n",
    "            'vader_sentiment': item_feats.loc[item_id]['vader_sentiment'],\n",
    "        }\n",
    "        candidate_rows.append(row)\n",
    "    labels.extend([1] + [0] * 99)\n",
    "\n",
    "# Build batch dataframe\n",
    "candidate_df = pd.DataFrame(candidate_rows)\n",
    "\n",
    "# Match categorical types\n",
    "for cat in ['user_id', 'gmap_id', 'time_of_day']:\n",
    "    candidate_df[cat] = candidate_df[cat].astype(pd.api.types.CategoricalDtype(categories=df[cat].cat.categories))\n",
    "\n",
    "print(\"\\n Predicting on batch...\")\n",
    "scores = model.predict(candidate_df[features])\n",
    "candidate_df['score'] = scores\n",
    "candidate_df['label'] = labels\n",
    "\n",
    "print(\"\\n Evaluating...\")\n",
    "grouped = candidate_df.groupby('user_id')\n",
    "\n",
    "for user_id, group in tqdm(grouped, desc=\"Scoring users\"):\n",
    "    group = group.sort_values('score', ascending=False)\n",
    "    ranked_labels = group['label'].values\n",
    "\n",
    "    for k in K_list:\n",
    "        top_k = ranked_labels[:k]\n",
    "        hit = int(1 in top_k)\n",
    "\n",
    "        pos_indices = np.where(ranked_labels == 1)[0]\n",
    "        if len(pos_indices) == 0:\n",
    "            continue  # skip users with no positive item\n",
    "\n",
    "        rank = pos_indices[0] + 1\n",
    "        ndcg = 1.0 / np.log2(rank + 1) if rank <= k else 0\n",
    "        precision = top_k.sum() / k\n",
    "        recall = 1.0 if 1 in top_k else 0\n",
    "\n",
    "        results_by_k[k]['recall'].append(recall)\n",
    "        results_by_k[k]['precision'].append(precision)\n",
    "        results_by_k[k]['hit'].append(hit)\n",
    "        results_by_k[k]['ndcg'].append(ndcg)\n",
    "\n",
    "\n",
    "print(\"\\n Final Ranking Metrics:\")\n",
    "for k in K_list:\n",
    "    print(f\"@{k}: Recall={np.mean(results_by_k[k]['recall']):.4f}, \"\n",
    "          f\"Precision={np.mean(results_by_k[k]['precision']):.4f}, \"\n",
    "          f\"HitRate={np.mean(results_by_k[k]['hit']):.4f}, \"\n",
    "          f\"NDCG={np.mean(results_by_k[k]['ndcg']):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862aadb",
   "metadata": {},
   "source": [
    "Final Ranking Metrics:\n",
    "\n",
    "@5: Recall=0.2365, Precision=0.0473, HitRate=0.2365, NDCG=0.1416\n",
    "\n",
    "@10: Recall=0.2880, Precision=0.0288, HitRate=0.2880, NDCG=0.1586\n",
    "\n",
    "@20: Recall=0.3132, Precision=0.0157, HitRate=0.3132, NDCG=0.1648"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
